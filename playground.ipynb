{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T17:24:43.041850Z",
     "start_time": "2025-07-17T17:24:42.310517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import librosa\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "# from src.data.preproc import convert_libri\n",
    "from src.data.util import log_compress, read_audioset_csv"
   ],
   "id": "ae60e99e85bfcc9f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T21:21:29.002658Z",
     "start_time": "2025-07-17T21:21:28.997726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_flac = '/Users/lkieu/Desktop/Audioset/processed/-5-vmt2iKT0.flac'\n",
    "\n",
    "waveform, sr = torchaudio.load(test_flac, normalize=True)\n"
   ],
   "id": "bbdf2be2fbb248b7",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T17:57:58.765589Z",
     "start_time": "2025-07-17T17:57:58.763266Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 18,
   "source": "waveform = waveform.squeeze()",
   "id": "ed90a5bbc79e8b5c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T21:21:31.352333Z",
     "start_time": "2025-07-17T21:21:31.188252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyannote.audio.pipelines import VoiceActivityDetection\n",
    "# instantiate the model\n",
    "from pyannote.audio import Model\n",
    "model = Model.from_pretrained(\n",
    "  \"pyannote/segmentation-3.0\")\n",
    "pipeline = VoiceActivityDetection(segmentation=model)\n",
    "HYPER_PARAMETERS = {\n",
    "  # remove speech regions shorter than that many seconds.\n",
    "  \"min_duration_on\": 0.0,\n",
    "  # fill non-speech regions shorter than that many seconds.\n",
    "  \"min_duration_off\": 0.0\n",
    "}\n",
    "pipeline.instantiate(HYPER_PARAMETERS)\n",
    "vad = pipeline({'waveform': waveform, 'sample_rate': sr})\n",
    "# `vad` is a pyannote.core.Annotation instance containing speech regions\n",
    "str(vad)"
   ],
   "id": "ba117b31733a6759",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[ 00:00:00.030 -->  00:00:09.480] 0 SPEECH'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T18:01:32.187687Z",
     "start_time": "2025-07-17T18:01:28.789297Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 21,
   "source": [
    "from transformers import Wav2Vec2ForSequenceClassification, AutoFeatureExtractor\n",
    "import torch\n",
    "\n",
    "model_id = \"facebook/mms-lid-256\"\n",
    "\n",
    "processor = AutoFeatureExtractor.from_pretrained(model_id)\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(model_id)\n",
    "# English\n",
    "inputs = processor(waveform, sampling_rate=16_000, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs).logits\n",
    "\n",
    "lang_id = torch.argmax(outputs, dim=-1)[0].item()\n",
    "detected_lang = model.config.id2label[lang_id]\n",
    "# 'eng'\n"
   ],
   "id": "a235d071ef4d61f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T18:01:36.513701Z",
     "start_time": "2025-07-17T18:01:36.511275Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lat'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22,
   "source": "detected_lang",
   "id": "ca93d2d2e586b0b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T20:14:50.002169Z",
     "start_time": "2025-07-17T20:14:49.977480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path = '/Users/lkieu/PycharmProjects/PhonemeAwareFoundational/test_audio/balanced_train_segments.csv'\n",
    "df = read_audioset_csv(path)\n",
    "df.head(3)"
   ],
   "id": "77dd223c86314309",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          YTID  start_seconds  end_seconds                 positive_labels\n",
       "0  --PJHxphWEs           30.0         40.0          [/m/09x0r, /t/dd00088]\n",
       "1  --ZhevVpy1s           50.0         60.0                     [/m/012xff]\n",
       "2  --aE2O5G5WE            0.0         10.0  [/m/03fwl, /m/04rlf, /m/09x0r]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YTID</th>\n",
       "      <th>start_seconds</th>\n",
       "      <th>end_seconds</th>\n",
       "      <th>positive_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--PJHxphWEs</td>\n",
       "      <td>30.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>[/m/09x0r, /t/dd00088]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--ZhevVpy1s</td>\n",
       "      <td>50.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>[/m/012xff]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>--aE2O5G5WE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>[/m/03fwl, /m/04rlf, /m/09x0r]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T20:14:38.297239Z",
     "start_time": "2025-07-17T20:14:38.262900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.data.preproc import convert_audioset, has_allowed_tag\n",
    "\n",
    "audio_path = '/Users/lkieu/Desktop/Audioset/audio/bal_train'\n",
    "paths = convert_audioset(audio_path)\n",
    "\n",
    "# Filter for speech tags\n",
    "df = read_audioset_csv(path)\n",
    "ytids_to_paths = {\n",
    "        os.path.splitext(os.path.basename(p))[0]: p for p in paths\n",
    "    }\n",
    "\n",
    "ytids = list(ytids_to_paths.keys())\n",
    "df_filtered = df[df['YTID'].isin(ytids)].copy()\n",
    "df_filtered['has_allowed_tag'] = df_filtered['positive_labels'].apply(has_allowed_tag)\n",
    "df_result = df_filtered[df_filtered['has_allowed_tag']==True ].copy()\n",
    "len(df_result)"
   ],
   "id": "68e9ae1685d0d23b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "534"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T20:34:26.887802Z",
     "start_time": "2025-07-17T20:34:26.883181Z"
    }
   },
   "cell_type": "code",
   "source": "df_result.head(3)",
   "id": "f75f81c46f71c756",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           YTID  start_seconds  end_seconds                  positive_labels  \\\n",
       "0   --PJHxphWEs           30.0         40.0           [/m/09x0r, /t/dd00088]   \n",
       "2   --aE2O5G5WE            0.0         10.0   [/m/03fwl, /m/04rlf, /m/09x0r]   \n",
       "25  -30H9V1IKps            6.0         16.0  [/m/07yv9, /m/09x0r, /m/0gvgw0]   \n",
       "\n",
       "    has_allowed_tag  \n",
       "0              True  \n",
       "2              True  \n",
       "25             True  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YTID</th>\n",
       "      <th>start_seconds</th>\n",
       "      <th>end_seconds</th>\n",
       "      <th>positive_labels</th>\n",
       "      <th>has_allowed_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--PJHxphWEs</td>\n",
       "      <td>30.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>[/m/09x0r, /t/dd00088]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>--aE2O5G5WE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>[/m/03fwl, /m/04rlf, /m/09x0r]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-30H9V1IKps</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>[/m/07yv9, /m/09x0r, /m/0gvgw0]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Gather all the flac and json\n",
    "data_dir = \"./LibriLight/small\"\n",
    "\n",
    "def gather_flac_json_pairs(root_dir):\n",
    "    flac_files = glob.glob(os.path.join(root_dir, '**', '*.flac'), recursive=True)\n",
    "    pairs = []\n",
    "\n",
    "    for flac_path in flac_files:\n",
    "        json_path = os.path.splitext(flac_path)[0] + '.json'\n",
    "        if os.path.exists(json_path):\n",
    "            pairs.append((flac_path, json_path))\n",
    "        else:\n",
    "            print(f\"Warning: No JSON companion for {flac_path}\")\n",
    "\n",
    "    return pairs\n",
    "\n",
    "pairs = gather_flac_json_pairs(data_dir)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a5d5e38de472e6f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "def find_duplicate_filenames(paths):\n",
    "    filename_to_paths = defaultdict(list)\n",
    "\n",
    "    for path in paths:\n",
    "        filename = os.path.basename(path)\n",
    "        filename_to_paths[filename].append(path)\n",
    "\n",
    "    duplicates = {fname: plist for fname, plist in filename_to_paths.items() if len(plist) > 1}\n",
    "\n",
    "    for fname, plist in duplicates.items():\n",
    "        print(f\"Duplicate filename: {fname}\")\n",
    "        for p in plist:\n",
    "            print(f\"  {p}\")\n",
    "\n",
    "paths = list(map(lambda x: x[0], pairs))\n",
    "find_duplicate_filenames(paths)\n"
   ],
   "id": "94ee1b3aaf636bfa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Verify Sampling rate\n",
    "def get_sample_rate(path):\n",
    "    metadata = sf.info(path)\n",
    "    return metadata.samplerate\n",
    "\n",
    "sample_rate = {}\n",
    "for path, _ in pairs:\n",
    "    info = f'{get_sample_rate(path)} hz'\n",
    "    if info not in sample_rate:\n",
    "        sample_rate[info] = 1\n",
    "    else:\n",
    "        sample_rate[info] += 1\n",
    "print(sample_rate)"
   ],
   "id": "714036b28fa0d6cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Distribution of Voice Activity block length.\n",
    "# Specific to LibriLight\n",
    "from collections import Counter\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def collect_duration_differences(flac_json_pairs):\n",
    "    all_durations = []\n",
    "\n",
    "    for flac_path, json_path in flac_json_pairs:\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        voice_activity = data.get('voice_activity', [])\n",
    "        durations = [end - start for start, end in voice_activity]\n",
    "        all_durations.extend(durations)\n",
    "\n",
    "    return all_durations\n",
    "\n",
    "def plot_duration_distribution(durations, bins=50):\n",
    "    plt.hist(durations, bins=bins, edgecolor='black')\n",
    "    plt.title('Distribution of Voice Activity Durations')\n",
    "    plt.xlabel('Duration (seconds)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "def top_n_durations(durations, n=10, rounding=2):\n",
    "    rounded_durations = [round(d, rounding) for d in durations]\n",
    "    counter = Counter(rounded_durations)\n",
    "    most_common = counter.most_common(n)\n",
    "    return most_common\n",
    "\n",
    "\n",
    "durations = collect_duration_differences(pairs)\n",
    "print(top_n_durations(durations))\n",
    "print('min: ' + str(min(durations)))\n",
    "print('max: ' + str(max(durations)))\n",
    "plot_duration_distribution(durations)"
   ],
   "id": "19910aa415b7ef85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# What if we allow for silence of 1s\n",
    "\n",
    "def merge_close_blocks(voice_activity, threshold=1.0):\n",
    "    if not voice_activity:\n",
    "        return []\n",
    "\n",
    "    # Sort by start time just in case\n",
    "    voice_activity = sorted(voice_activity, key=lambda x: x[0])\n",
    "    merged = [voice_activity[0]]\n",
    "\n",
    "    for start, end in voice_activity[1:]:\n",
    "        last_start, last_end = merged[-1]\n",
    "        if start - last_end < threshold:\n",
    "            # Merge intervals\n",
    "            merged[-1][1] = max(last_end, end)\n",
    "        else:\n",
    "            merged.append([start, end])\n",
    "\n",
    "    return merged\n",
    "\n",
    "def get_duration_diff_merged(pairs):\n",
    "    all_durations = []\n",
    "\n",
    "    for flac_path, json_path in pairs:\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        voice_activity = data.get('voice_activity', [])\n",
    "        voice_activity = merge_close_blocks(voice_activity)\n",
    "        durations = [end - start for start, end in voice_activity]\n",
    "        all_durations.extend(durations)\n",
    "\n",
    "    return all_durations\n",
    "\n",
    "durations = get_duration_diff_merged(pairs)\n",
    "print(top_n_durations(durations))\n",
    "print('min: ' + str(min(durations)))\n",
    "print('max: ' + str(max(durations)))\n",
    "plot_duration_distribution(durations)"
   ],
   "id": "92d9b7523980b4f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "a = torch.tensor([[1,2],\n",
    "                  [3,4]])\n",
    "b = torch.tensor([[5,6],\n",
    "                 [7,8]])\n",
    "torch.maximum(a, a.max() - 1)"
   ],
   "id": "39956b310a7a9918",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torchaudio import transforms\n",
    "import torchaudio\n",
    "\n",
    "test_flac = '/Users/lkieu/PycharmProjects/PhonemeAwareFoundational/test_audio/canterburytales_09_chaucer_64kb.flac'\n",
    "waveform, sample_rate = torchaudio.load(test_flac, normalize=True)\n",
    "transform = transforms.MelSpectrogram(sample_rate=sample_rate, n_fft=1024, hop_length=256, n_mels=80)\n",
    "melspec = transform(waveform)\n",
    "# Define max size (in time frames)\n",
    "max_frames = 500  # Example\n",
    "\n",
    "def crop_or_pad(spec, max_frames):\n",
    "    channels, n_mels, time_frames = spec.shape\n",
    "    if time_frames > max_frames:\n",
    "        return spec[:, :, :max_frames]\n",
    "    elif time_frames < max_frames:\n",
    "        pad_amount = max_frames - time_frames\n",
    "        pad = torch.zeros((channels, n_mels, pad_amount), device=spec.device)\n",
    "        return torch.cat((spec, pad), dim=2)\n",
    "    else:\n",
    "        return spec\n",
    "\n",
    "melspec= crop_or_pad(melspec, max_frames)\n",
    "print(melspec.shape)  # Should be (channels, n_mels, max_frames)\n"
   ],
   "id": "f0cb4bcf1b148a21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.data.util import log_compress\n",
    "import librosa\n",
    "\n",
    "log_spec_lib = librosa.power_to_db(melspec[0])\n",
    "log_spec_torch = log_compress(melspec[0])"
   ],
   "id": "d312169be689a2e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_spectrogram(specgram, title=None, ylabel=\"freq_bin\", ax=None):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(1, 1)\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.imshow(specgram, origin=\"lower\", aspect=\"auto\", interpolation=\"nearest\")"
   ],
   "id": "379c5a1292d81047",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_spectrogram(log_spec_lib)",
   "id": "1a31040cfa40540e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_spectrogram(log_spec_torch)",
   "id": "a758f58022bfc890",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.data.util import whisper_norm, z_score_norm, min_max_norm\n",
    "whisper_norm_log_spec = whisper_norm(log_spec_torch)\n",
    "plot_spectrogram(whisper_norm_log_spec)"
   ],
   "id": "961e678b51634d2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "z_norm = z_score_norm(log_spec_torch)\n",
    "plot_spectrogram(z_norm)"
   ],
   "id": "5edd68455c7250d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "min_max_norm_log_spec = min_max_norm(log_spec_torch)\n",
    "plot_spectrogram(min_max_norm_log_spec)"
   ],
   "id": "7d0b44ecf567244e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e9ee8950cd0ddd2b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
